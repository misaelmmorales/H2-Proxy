{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneity Image Translation Transformer\n",
    "\n",
    "### Multiscale Residual Spatiotemporal Vision Transformer (MRSt-ViT | PixFormer)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': device(type='cuda'),\n",
       " 'verbose': True,\n",
       " 'folder': 'Fdataset',\n",
       " 'lr': 0.001,\n",
       " 'weight_decay': 1e-05,\n",
       " 'mse_weight': 1.0,\n",
       " 'ssim_weight': 1.0,\n",
       " 'train_perc': 0.75,\n",
       " 'valid_perc': 0.1,\n",
       " 'batch_size': 32,\n",
       " 'num_epochs': 100}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import *\n",
    "\n",
    "hete = Heterogeneity()\n",
    "hete.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "Torch version: 2.1.0+cu121\n",
      "Torch build with CUDA? True\n",
      "# Device(s) available: 1, Name(s): NVIDIA GeForce RTX 3080\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hete.check_torch_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = hete.make_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Total number of trainable parameters: 185,746,560\n"
     ]
    }
   ],
   "source": [
    "model, train_loss, valid_loss, train_ssim, valid_ssim = hete.trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import SqueezeExcitation\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    '''\n",
    "    Define custom loss function: L = a*MSE + b*(1-SSIM)\n",
    "    '''\n",
    "    def __init__(self, mse_weight=1.0, ssim_weight=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mse_weight  = mse_weight                                            # weights for MSE\n",
    "        self.ssim_weight = ssim_weight                                           # weights for SSIM\n",
    "        self.mse_loss    = nn.MSELoss()                                          # Mean Squared Error\n",
    "        self.ssim        = SSIM().to(self.device)                                # Structural Similarity Index Measure\n",
    "    def forward(self, pred, target):\n",
    "        mse_loss   = self.mse_loss(pred, target)                                 # mse loss\n",
    "        ssim_loss  = 1.0 - self.ssim(pred, target)                               # ssim loss\n",
    "        total_loss = self.mse_weight * mse_loss + self.ssim_weight * ssim_loss   # combined loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    '''\n",
    "    Patchify the input image into patches for vision transformer\n",
    "    '''\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size                           # get original image size\n",
    "        self.patch_size = patch_size                           # get user-defined patch size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        patches = self.projection(x)                           # convolve image to patch\n",
    "        patches = rearrange(patches, 'b c h w -> b (h w) c')   # rearrange patches\n",
    "        return patches\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    Get the positional codes for each patch of the input image\n",
    "    '''\n",
    "    def __init__(self, embed_dim, max_seq_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()   # get position indices\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "        pos_enc  = torch.zeros((1, max_seq_len, embed_dim))            # instantiate empty tensor\n",
    "        pos_enc[0, :, 0::2] = torch.sin(position * div_term)           # compute positional encoding sine\n",
    "        pos_enc[0, :, 1::2] = torch.cos(position * div_term)           # compute positional encoding cosine\n",
    "        self.register_buffer('pos_enc', pos_enc)                       # register buffer for positional encoding\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_enc[:, :x.size(1)].detach()                # add positional encoding to patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    QKV MultiHead Attention mechanism\n",
    "    '''\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = embed_dim // num_heads                                                         # required: embed_dim/num_heads\n",
    "        self.query  = nn.Linear(embed_dim, embed_dim)                                                   # querys\n",
    "        self.key    = nn.Linear(embed_dim, embed_dim)                                                   # keys\n",
    "        self.value  = nn.Linear(embed_dim, embed_dim)                                                   # values\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)                                                   # outputs\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)   # calculate query\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)       # calculate keys\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)   # calculate values\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)) # get scores\n",
    "        attention_weights = F.softmax(scores, dim=-1)                                                   # get attention weights\n",
    "        out = torch.matmul(attention_weights, V)                                                        # calculate output \n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embed_dim)                 # rearrange output\n",
    "        out = self.fc_out(out)                                                                          # compute output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    '''\n",
    "    Multi-Layer Perceptron block for vision transformer\n",
    "    '''\n",
    "    def __init__(self, embed_dim, mlp_hidden_dim):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, mlp_hidden_dim)\n",
    "        self.fc2 = nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))   # activate outputs\n",
    "        x = self.fc2(x)           # compute outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    '''\n",
    "    Single ViT block with attention\n",
    "    '''\n",
    "    def __init__(self, embed_dim, num_heads, mlp_hidden_dim=1024):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(embed_dim, num_heads)   # Attention mechanism\n",
    "        self.mlp_block = MLPBlock(embed_dim, mlp_hidden_dim)             # MLP block\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)                             # normalization 1\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)                             # normalization 2\n",
    "    def forward(self, x):\n",
    "        attention_output = self.self_attention(x, x, x)                  # attention values\n",
    "        x = x + attention_output                                         # update attention\n",
    "        x = self.norm1(x)                                                # normalize attention\n",
    "        mlp_output = self.mlp_block(x)                                   # apply MLP block\n",
    "        x = x + mlp_output                                               # update MLP outputs\n",
    "        x = self.norm2(x)                                                # normalize outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTencoder(nn.Module):\n",
    "    '''\n",
    "    Single ViT block with patch embedding and positional encoding\n",
    "    '''\n",
    "    def __init__(self, image_size=256, latent_size=32, in_channels=3, patch_size=16, projection_dim=256, embed_dim=1024, num_heads=16, num_layers=8):\n",
    "        super(ViTencoder, self).__init__()\n",
    "        self.patch_embedding     = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)   # patch embedding\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)                                    # positional encoding\n",
    "        self.transformer_blocks  = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads) for _ in range(num_layers)])              # N Transfromer blocks\n",
    "        self.global_avg_pooling  = nn.AdaptiveAvgPool1d(1)                                          # global average pooling\n",
    "        self.fc = nn.Linear(embed_dim, projection_dim*latent_size*latent_size)                      # fully connected layer\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)                                                                 # patch embedding\n",
    "        x = self.positional_encoding(x)                                                             # positional encoding\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)                                                                # transformer block(s)\n",
    "        x = self.global_avg_pooling(x.transpose(1, 2))                                              # global average pooling\n",
    "        x = x.squeeze(2)                                                                            # squeeze output\n",
    "        x = self.fc(x)                                                                              # activate outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleResidual(nn.Module):\n",
    "    '''\n",
    "    Multiscale Residual concatenation block\n",
    "    '''\n",
    "    def __init__(self, image_size=256):\n",
    "        super(MultiScaleResidual, self).__init__()\n",
    "        self.image_size = image_size                                     # original image size\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.shape                                             # get image dimensions\n",
    "        scale = [self.image_size//h, self.image_size//w]                 # get scale factors\n",
    "        size  = (self.image_size//scale[0], self.image_size//scale[1])   # get upscaled image size\n",
    "        x_ups = transforms.Resize(size, antialias=True)(x)               # resize original image to upscaled size\n",
    "        return torch.cat([x, x_ups], dim=1)                              # concatenate image and upscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixFormer(nn.Module):\n",
    "    '''\n",
    "    PixFormer model: \n",
    "    (1) Vision Transformer encoder\n",
    "    (2) Multiscale Residual Spatiotemporal decoder\n",
    "    '''\n",
    "    def __init__(self, projection_dim=128, latent_size=32):\n",
    "        super(PixFormer, self).__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.latent_size    = latent_size\n",
    "        self.encoder = ViTencoder(latent_size=latent_size, projection_dim=projection_dim)   # Encoder block\n",
    "        self.layers = nn.Sequential(\n",
    "            self._conv_block(projection_dim, projection_dim//2),                 # first decoder layer\n",
    "            self._conv_block(projection_dim//2, projection_dim//4),              # second decoder layer\n",
    "            self._conv_block(projection_dim//4, projection_dim//8))              # third decoder layer\n",
    "        self.out = nn.Conv2d(projection_dim//8, 1, kernel_size=3, padding=1)     # output layer\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),      # convolve inputs\n",
    "            SqueezeExcitation(out_channels, out_channels//4),                    # Squeeze-and-Excite for multichannel feature maps\n",
    "            nn.InstanceNorm2d(out_channels),                                     # Normalize by instance\n",
    "            nn.PReLU(),                                                          # Parametric ReLU activation\n",
    "            MultiScaleResidual(),                                                # Multi-scale residual concatenation\n",
    "            nn.Upsample(scale_factor=2),                                         # Upsample by 2x\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1))   # convolve concatenated features\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)                                                       # encode inputs: z = Enc(x)\n",
    "        x = x.view(-1, self.projection_dim, self.latent_size, self.latent_size)   # reshape z: vector -> feature maps\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)                                                          # decode inputs: y = Dec(z)\n",
    "        x_output = self.out(x)                                                    # output layer\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    Generate a custom dataset from .npz files\n",
    "    (x) porosity, permeability, timesteps\n",
    "    (y) pressure, saturation\n",
    "    '''\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.file_paths[idx])                                              # load .npz file in file_paths\n",
    "        poro = np.tile(data['poro'], (60, 1, 1, 1))                                       # reshape porosity channel\n",
    "        perm = np.tile(data['perm'], (60, 1, 1, 1))                                       # reshape permeability channel\n",
    "        timesteps = np.tile(np.arange(1, 61).reshape(60, 1, 1, 1), (1, 1, 256, 256))      # construct time channel\n",
    "        pres = data['pres'].reshape(60, 1, 256, 256)                                      # reshape pressure channel\n",
    "        sat = data['sat'].reshape(60, 1, 256, 256)                                        # reshape saturation channel\n",
    "        X_data = np.concatenate([poro, perm, timesteps], axis=1).reshape(-1, 3, 256, 256) # Inputs  (X)\n",
    "        y_data = np.concatenate([pres, sat], axis=1).reshape(-1, 2, 256, 256)             # Outputs (y)\n",
    "        return torch.Tensor(X_data), torch.Tensor(y_data)                                 # Tensorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataLoader(DataLoader):\n",
    "    '''\n",
    "    Generate a custom dataloader for dataset\n",
    "    (train): x,y at timesteps 0-40\n",
    "    (valid): x,y at timesteps 40-50\n",
    "    (test):  x,y at timesteps 50-60\n",
    "    '''\n",
    "    def __init__(self, *args, mode:str=None, **kwargs):\n",
    "        super(MyDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.mode = mode\n",
    "    def __iter__(self):\n",
    "        for batch in super(MyDataLoader, self).__iter__():\n",
    "            X_data, y_data = batch          # loads a batch of data with shate (b, t, c, h, w)\n",
    "            if self.mode == 'train':        # _____TRAINING_____\n",
    "                X_data = X_data[:, :40]     # x at timesteps 0-40\n",
    "                y_data = y_data[:, :40]     # y at timesteps 0-40\n",
    "            elif self.mode == 'valid':      # _____VALIDATION_____\n",
    "                X_data = X_data[:, 40:50]   # x at timesteps 40-50\n",
    "                y_data = y_data[:, 40:50]   # y at timesteps 40-50\n",
    "            elif self.mode == 'test':       # ______TESTING______\n",
    "                X_data = X_data[:, 50:]     # x at timesteps 50-60\n",
    "                y_data = y_data[:, 50:]     # y at timesteps 50-60\n",
    "            else:\n",
    "                raise ValueError('Invalid mode: {} | select between \"train\", \"valid\" or \"test\"'.format(self.mode))\n",
    "            X_data = X_data.reshape(-1, X_data.size(2), X_data.size(3), X_data.size(4)) # reshape to (b*t, c, h, w)\n",
    "            y_data = y_data.reshape(-1, y_data.size(2), y_data.size(3), y_data.size(4)) # reshape to (b*t, c, h, w)\n",
    "            yield X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
