{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from HeteTrans import *\n",
    "\n",
    "hete = Heterogeneity()\n",
    "hete.check_torch_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, math, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista as pv\n",
    "from time import time\n",
    "\n",
    "from cv2 import resize\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import loadmat, savemat\n",
    "from numpy.matlib import repmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeTransform:\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        x_normalized = self.normalize_data(x)\n",
    "        y_normalized = self.normalize_data(y)\n",
    "        return x_normalized, y_normalized\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        # Assuming data is a PyTorch tensor\n",
    "        scaler = MinMaxScaler()\n",
    "        data_np = data.numpy()\n",
    "        data_normalized_np = scaler.fit_transform(data_np.reshape(-1, data_np.shape[-1])).reshape(data_np.shape)\n",
    "        return torch.Tensor(data_normalized_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.y_folder = data_folder\n",
    "        self.x_folder = os.path.join(data_folder, 'X_data')\n",
    "        self.y_list = [file for file in os.listdir(self.y_folder) if file.endswith('.mat')]\n",
    "        self.x_list = os.listdir(self.x_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_name = self.x_list[idx]\n",
    "        y_name = self.y_list[idx]\n",
    "\n",
    "        x_path = os.path.join(self.x_folder, x_name)\n",
    "        y_path = os.path.join(self.y_folder, y_name)\n",
    "\n",
    "        x_data = np.load(x_path)\n",
    "        poro = np.expand_dims(x_data[0], 0)\n",
    "        perm = np.expand_dims(np.log10(x_data[1]), 0)\n",
    "        t = ((torch.ones((256, 256, 61)) * torch.arange(61)).T).unsqueeze(0)\n",
    "        x = torch.Tensor(np.concatenate([poro, perm], 0)).unsqueeze(1).repeat(1, 61, 1, 1)\n",
    "        x = torch.cat([x, t], dim=0)\n",
    "\n",
    "        y = torch.zeros((2, 61, 256, 256))\n",
    "        y_data = loadmat(y_path, simplify_cells=True)\n",
    "        for timestep in range(61):\n",
    "            y[0, timestep] = torch.Tensor(y_data['PRESSURE'])\n",
    "            y[1, timestep] = torch.Tensor(y_data['SGAS'] * y_data['YMF_3'])\n",
    "\n",
    "        sample = (x, y)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batchnorm  = nn.BatchNorm2d(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x, x, x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_heads, num_layers):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding    = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.transformer_layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers) ])\n",
    "        self.fc                 = nn.Linear(embed_dim, in_channels)\n",
    "        self.batchnorm          = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        x = x.permute(1, 2, 0).view(x.size(1), -1, x.size(0))\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.gelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform  = NormalizeTransform()\n",
    "dataset    = MyDataset(data_folder='h2dataf', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "model      = VisionTransformer(in_channels=3, patch_size=16, embed_dim=256, num_heads=8, num_layers=4)\n",
    "criterion  = nn.MSELoss()\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs   = 10\n",
    "train_tsteps = 40 \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (xbatch, ybatch) in enumerate(dataloader):\n",
    "        # Flatten xbatch and ybatch for the Vision Transformer\n",
    "        xbatch = xbatch[:, :, :train_tsteps, :, :]\n",
    "        ybatch = ybatch[:, :, :train_tsteps, :, :]\n",
    "\n",
    "        xbatch = xbatch.reshape(-1, 3, 256, 256)\n",
    "        ybatch = ybatch.reshape(-1, 2, train_tsteps, 256, 256)\n",
    "\n",
    "        outputs = model(xbatch)\n",
    "        outputs = outputs.reshape(xbatch.size(0), 2, 40, 256, 256)\n",
    "\n",
    "        loss = criterion(outputs, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch: [{}/{}] | Batch: [{}/{}] | Loss: {}'.format(epoch+1, num_epochs, i+1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "plt.figure(figsize=(20,6))\n",
    "for i in range(3):\n",
    "    for j in range(12):\n",
    "        plt.subplot(3, 12, k+1)\n",
    "        im = plt.imshow(xbatch[55, i, j*5], cmap='jet')\n",
    "        plt.colorbar(im, pad=0.04, fraction=0.046)\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        k += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
