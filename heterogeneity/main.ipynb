{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "------------------ VERSION INFO -----------------\n",
      "Conda Environment: Python39 | Python version: 3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch version: 2.0.0+cu117\n",
      "Torch build with CUDA? True\n",
      "# Device(s) available: 1, Name(s): NVIDIA GeForce RTX 3080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from HeteTrans import *\n",
    "\n",
    "hete = Heterogeneity()\n",
    "hete.check_torch_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, math, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista as pv\n",
    "from time import time\n",
    "\n",
    "from cv2 import resize\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import loadmat, savemat\n",
    "from numpy.matlib import repmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeTransform:\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        x_normalized = self.normalize_data(x)\n",
    "        y_normalized = self.normalize_data(y)\n",
    "        return x_normalized, y_normalized\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        # Assuming data is a PyTorch tensor\n",
    "        scaler = MinMaxScaler()\n",
    "        data_np = data.numpy()\n",
    "        data_normalized_np = scaler.fit_transform(data_np.reshape(-1, data_np.shape[-1])).reshape(data_np.shape)\n",
    "        return torch.Tensor(data_normalized_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.y_folder = data_folder\n",
    "        self.x_folder = os.path.join(data_folder, 'X_data')\n",
    "        self.y_list = [file for file in os.listdir(self.y_folder) if file.endswith('.mat')]\n",
    "        self.x_list = os.listdir(self.x_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_name = self.x_list[idx]\n",
    "        y_name = self.y_list[idx]\n",
    "\n",
    "        x_path = os.path.join(self.x_folder, x_name)\n",
    "        y_path = os.path.join(self.y_folder, y_name)\n",
    "\n",
    "        x_data = np.load(x_path)\n",
    "        poro = np.expand_dims(resize(x_data[0], (64,64)),0)\n",
    "        perm = np.expand_dims(resize(np.log10(x_data[1]), (64,64)),0)\n",
    "        t = ((torch.ones((64, 64, 61)) * torch.arange(61)).T).unsqueeze(0)\n",
    "        x = torch.Tensor(np.concatenate([poro, perm], 0)).unsqueeze(1).repeat(1, 61, 1, 1)\n",
    "        x = torch.cat([x, t], dim=0)\n",
    "\n",
    "        y = torch.zeros((2, 61, 64, 64))\n",
    "        y_data = loadmat(y_path, simplify_cells=True)\n",
    "        for timestep in range(61):\n",
    "            y[0, timestep] = torch.Tensor(resize(y_data['PRESSURE'], (64,64)))\n",
    "            y[1, timestep] = torch.Tensor(resize(y_data['SGAS'],(64,64)) * resize(y_data['YMF_3'], (64,64)))\n",
    "\n",
    "        sample = (x, y)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batchnorm  = nn.BatchNorm2d(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x, x, x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_heads, num_layers):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding    = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.transformer_layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers) ])\n",
    "        self.fc                 = nn.Linear(embed_dim, in_channels)\n",
    "        self.batchnorm          = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        x = x.permute(1, 2, 0).view(x.size(1), -1, x.size(0))\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.gelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misael Morales\\AppData\\Local\\Temp\\ipykernel_13216\\2477700124.py:22: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  t = ((torch.ones((64, 64, 61)) * torch.arange(61)).T).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512000x16 and 256x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\H2-Proxy\\heterogeneity\\main.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m xbatch \u001b[39m=\u001b[39m xbatch\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ybatch \u001b[39m=\u001b[39m ybatch\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, train_tsteps, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(xbatch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mreshape(xbatch\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m, \u001b[39m40\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, ybatch)\n",
      "File \u001b[1;32mc:\\Users\\Misael Morales\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32me:\\H2-Proxy\\heterogeneity\\main.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchnorm(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/H2-Proxy/heterogeneity/main.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Misael Morales\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Misael Morales\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512000x16 and 256x3)"
     ]
    }
   ],
   "source": [
    "transform  = NormalizeTransform()\n",
    "dataset    = MyDataset(data_folder='h2dataf', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "model      = VisionTransformer(in_channels=3, patch_size=16, embed_dim=256, num_heads=8, num_layers=4)\n",
    "criterion  = nn.MSELoss()\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs   = 10\n",
    "train_tsteps = 40 \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (xbatch, ybatch) in enumerate(dataloader):\n",
    "        xbatch = xbatch[:, :, :train_tsteps, :, :]\n",
    "        ybatch = ybatch[:, :, :train_tsteps, :, :]\n",
    "\n",
    "        xbatch = xbatch.reshape(-1, 3, 64, 64)\n",
    "        ybatch = ybatch.reshape(-1, 2, train_tsteps, 64, 64)\n",
    "\n",
    "        outputs = model(xbatch)\n",
    "        outputs = outputs.reshape(xbatch.size(0), 2, 40, 64, 64)\n",
    "\n",
    "        loss = criterion(outputs, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch: [{}/{}] | Batch: [{}/{}] | Loss: {}'.format(epoch+1, num_epochs, i+1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "plt.figure(figsize=(20,6))\n",
    "for i in range(3):\n",
    "    for j in range(12):\n",
    "        plt.subplot(3, 12, k+1)\n",
    "        im = plt.imshow(xbatch[55, i, j*5], cmap='jet')\n",
    "        plt.colorbar(im, pad=0.04, fraction=0.046)\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        k += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
