{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from HeteTrans import *\n",
    "\n",
    "hete = Heterogeneity()\n",
    "hete.check_torch_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misaelmorales/miniconda3/envs/torchy/lib/python3.10/site-packages/vformer-0.1.3-py3.10.egg/vformer/models/classification/cvt.py:121: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/misaelmorales/miniconda3/envs/torchy/lib/python3.10/site-packages/vformer-0.1.3-py3.10.egg/vformer/models/classification/cvt.py:121: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv2 import resize\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from vformer.models.classification.vivit import ViViTModel2, ViViTEncoder\n",
    "from vformer.encoder.embedding.video_patch_embeddings import TubeletEmbedding, LinearVideoEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xbatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/e/H2-Proxy/heterogeneity/main.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(xbatch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(ybatch\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xbatch' is not defined"
     ]
    }
   ],
   "source": [
    "print(xbatch.shape)\n",
    "print(ybatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TubeletEmbedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/e/H2-Proxy/heterogeneity/main.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m TubeletEmbedding(embedding_dim\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, tubelet_t\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, tubelet_h\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, tubelet_w\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, in_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m z \u001b[39m=\u001b[39m model(xbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m z \u001b[39m=\u001b[39m model(z)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TubeletEmbedding' is not defined"
     ]
    }
   ],
   "source": [
    "model = TubeletEmbedding(embedding_dim=1024, tubelet_t=6, tubelet_h=8, tubelet_w=8, in_channels=3)\n",
    "\n",
    "z = model(xbatch)\n",
    "z = model(z)\n",
    "z = z.view(z.shape[0], z.shape[1], z.shape[2], int(np.sqrt(z.shape[3])), int(np.sqrt((z.shape[3]))))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 15, 129, 65, 65])\n",
      "torch.Size([50, 15, 97, 65, 65])\n",
      "-----------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "w = nn.ConvTranspose3d(5, 15, 3, padding=0, stride=2)(z)\n",
    "print(w.shape)\n",
    "w = nn.Conv3d(15, 15, (35,3,3), padding=1)(w)\n",
    "print(w.shape)\n",
    "\n",
    "print('-----------')\n",
    "\n",
    "w = nn.ConvTranspose3d(15, 30, 3, padding=0, stride=2)(w)\n",
    "print(w.shape)\n",
    "w = nn.Conv3d(30, 30, (19,3,3), padding=1)(w)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeTransform:\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        x_normalized = self.normalize_data(x)\n",
    "        y_normalized = self.normalize_data(y)\n",
    "        return x_normalized, y_normalized\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        scaler = MinMaxScaler()\n",
    "        data_np = data.numpy()\n",
    "        data_normalized_np = scaler.fit_transform(data_np.reshape(-1, data_np.shape[-1])).reshape(data_np.shape)\n",
    "        return torch.Tensor(data_normalized_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_folder, start, end, transform=None):\n",
    "        self.y_folder = data_folder\n",
    "        self.x_folder = os.path.join(data_folder, 'X_data')\n",
    "        self.y_list = [file for file in os.listdir(self.y_folder) if file.endswith('.mat')]\n",
    "        self.x_list = os.listdir(self.x_folder)\n",
    "        self.transform = transform\n",
    "        self.start, self.end = start, end\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_name = self.x_list[idx]\n",
    "        y_name = self.y_list[idx]\n",
    "\n",
    "        x_path = os.path.join(self.x_folder, x_name)\n",
    "        y_path = os.path.join(self.y_folder, y_name)\n",
    "\n",
    "        x_data = np.load(x_path)\n",
    "        poro = torch.tensor(resize(x_data[0], (64, 64)), dtype=torch.float32).unsqueeze(0).repeat(1, 61, 1, 1)\n",
    "        perm = torch.tensor(resize(np.log10(x_data[1]), (64, 64)), dtype=torch.float32).unsqueeze(0).repeat(1, 61, 1, 1)\n",
    "        time = torch.tensor(np.arange(61).reshape(1, 61, 1, 1), dtype=torch.float32).repeat(1, 1, 64, 64)\n",
    "        x = torch.cat([poro, perm, time], dim=0).permute(1,0,2,3)\n",
    "\n",
    "        y = torch.zeros((2, 61, 64, 64))\n",
    "        y_data = loadmat(y_path, simplify_cells=True)\n",
    "        for timestep in range(61):\n",
    "            y[0, timestep] = torch.tensor(resize(y_data['PRESSURE'], (64, 64)), dtype=torch.float32)\n",
    "            y[1, timestep] = torch.tensor(resize(y_data['SGAS'], (64, 64)) * resize(y_data['YMF_3'], (64, 64)), dtype=torch.float32)\n",
    "        y = y.permute(1,0,2,3)\n",
    "\n",
    "        sample = (x[self.start:self.end], y[self.start:self.end])\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 30, 3, 64, 64]) torch.Size([50, 30, 2, 64, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/e/H2-Proxy/heterogeneity/main.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m valid_loader \u001b[39m=\u001b[39m DataLoader(validdataset, batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m test_loader  \u001b[39m=\u001b[39m DataLoader(testdataset, batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (xbatch, ybatch) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m<\u001b[39m\u001b[39m2\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(xbatch\u001b[39m.\u001b[39mshape, ybatch\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchy/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchy/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchy/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/torchy/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/mnt/e/H2-Proxy/heterogeneity/main.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m timestep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m61\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     y[\u001b[39m0\u001b[39m, timestep] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(resize(y_data[\u001b[39m'\u001b[39m\u001b[39mPRESSURE\u001b[39m\u001b[39m'\u001b[39m], (\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     y[\u001b[39m1\u001b[39m, timestep] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(resize(y_data[\u001b[39m'\u001b[39;49m\u001b[39mSGAS\u001b[39;49m\u001b[39m'\u001b[39;49m], (\u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m)) \u001b[39m*\u001b[39m resize(y_data[\u001b[39m'\u001b[39m\u001b[39mYMF_3\u001b[39m\u001b[39m'\u001b[39m], (\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/e/H2-Proxy/heterogeneity/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m sample \u001b[39m=\u001b[39m (x[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend], y[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform  = NormalizeTransform()\n",
    "traindataset = MyDataset(data_folder='h2dataf', start=0, end=30)\n",
    "validdataset = MyDataset(data_folder='h2dataf', start=30, end=45)\n",
    "testdataset  = MyDataset(data_folder='h2dataf', start=45, end=-1)\n",
    "\n",
    "train_loader = DataLoader(traindataset, batch_size=50, shuffle=True)\n",
    "valid_loader = DataLoader(validdataset, batch_size=50, shuffle=True)\n",
    "test_loader  = DataLoader(testdataset, batch_size=50, shuffle=True)\n",
    "\n",
    "for i, (xbatch, ybatch) in enumerate(train_loader):\n",
    "    if i<2:\n",
    "        print(xbatch.shape, ybatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VivitModel, VivitConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vivit_config = VivitConfig(image_size=64, num_frames=30, hidden_size=1024, num_attention_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = VivitModel(vivit_config)(xbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vformer.encoder import SwinEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model      = ViTConvModel()\n",
    "criterion  = nn.MSELoss()\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs   = 10\n",
    "train_tsteps = 40 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print('Epoch: [{}/{}] | Batch: [{}/{}] | Loss: {}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_val_loss = 0.0\n",
    "    for x_val, y_val in valid_loader:\n",
    "        val_outputs = model(x_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "        total_val_loss += val_loss.item()\n",
    "    average_val_loss = total_val_loss / len(valid_loader)\n",
    "    print(f\"Validation Loss: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD ###\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batchnorm  = nn.BatchNorm2d(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x, x, x)[0]\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_heads, num_layers):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding    = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.transformer_layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers) ])\n",
    "        self.fc                 = nn.Linear(embed_dim, in_channels)\n",
    "        self.batchnorm          = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        x = x.permute(1, 2, 0).view(x.size(1), -1, x.size(0))\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.gelu(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
