{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from HeteTrans import *\n",
    "\n",
    "hete = Heterogeneity()\n",
    "hete.check_torch_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv2 import resize\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from vformer.models.classification.vivit import ViViTModel2, ViViTEncoder\n",
    "from vformer.encoder.embedding.video_patch_embeddings import TubeletEmbedding, LinearVideoEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xbatch.shape)\n",
    "print(ybatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TubeletEmbedding(embedding_dim=1024, tubelet_t=6, tubelet_h=8, tubelet_w=8, in_channels=3)\n",
    "\n",
    "z = model(xbatch)\n",
    "z = model(z)\n",
    "z = z.view(z.shape[0], z.shape[1], z.shape[2], int(np.sqrt(z.shape[3])), int(np.sqrt((z.shape[3]))))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.ConvTranspose3d(5, 15, 3, padding=0, stride=2)(z)\n",
    "print(w.shape)\n",
    "w = nn.Conv3d(15, 15, (35,3,3), padding=1)(w)\n",
    "print(w.shape)\n",
    "\n",
    "print('-----------')\n",
    "\n",
    "w = nn.ConvTranspose3d(15, 30, 3, padding=0, stride=2)(w)\n",
    "print(w.shape)\n",
    "w = nn.Conv3d(30, 30, (19,3,3), padding=1)(w)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeTransform:\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        x_normalized = self.normalize_data(x)\n",
    "        y_normalized = self.normalize_data(y)\n",
    "        return x_normalized, y_normalized\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        scaler = MinMaxScaler()\n",
    "        data_np = data.numpy()\n",
    "        data_normalized_np = scaler.fit_transform(data_np.reshape(-1, data_np.shape[-1])).reshape(data_np.shape)\n",
    "        return torch.Tensor(data_normalized_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_folder, start, end, transform=None):\n",
    "        self.y_folder = data_folder\n",
    "        self.x_folder = os.path.join(data_folder, 'X_data')\n",
    "        self.y_list = [file for file in os.listdir(self.y_folder) if file.endswith('.mat')]\n",
    "        self.x_list = os.listdir(self.x_folder)\n",
    "        self.transform = transform\n",
    "        self.start, self.end = start, end\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_name = self.x_list[idx]\n",
    "        y_name = self.y_list[idx]\n",
    "\n",
    "        x_path = os.path.join(self.x_folder, x_name)\n",
    "        y_path = os.path.join(self.y_folder, y_name)\n",
    "\n",
    "        x_data = np.load(x_path)\n",
    "        poro = torch.tensor(resize(x_data[0], (64, 64)), dtype=torch.float32).unsqueeze(0).repeat(1, 61, 1, 1)\n",
    "        perm = torch.tensor(resize(np.log10(x_data[1]), (64, 64)), dtype=torch.float32).unsqueeze(0).repeat(1, 61, 1, 1)\n",
    "        time = torch.tensor(np.arange(61).reshape(1, 61, 1, 1), dtype=torch.float32).repeat(1, 1, 64, 64)\n",
    "        x = torch.cat([poro, perm, time], dim=0).permute(1,0,2,3)\n",
    "\n",
    "        y = torch.zeros((2, 61, 64, 64))\n",
    "        y_data = loadmat(y_path, simplify_cells=True)\n",
    "        for timestep in range(61):\n",
    "            y[0, timestep] = torch.tensor(resize(y_data['PRESSURE'], (64, 64)), dtype=torch.float32)\n",
    "            y[1, timestep] = torch.tensor(resize(y_data['SGAS'], (64, 64)) * resize(y_data['YMF_3'], (64, 64)), dtype=torch.float32)\n",
    "        y = y.permute(1,0,2,3)\n",
    "\n",
    "        sample = (x[self.start:self.end], y[self.start:self.end])\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform  = NormalizeTransform()\n",
    "traindataset = MyDataset(data_folder='h2dataf', start=0, end=30)\n",
    "validdataset = MyDataset(data_folder='h2dataf', start=30, end=45)\n",
    "testdataset  = MyDataset(data_folder='h2dataf', start=45, end=-1)\n",
    "\n",
    "train_loader = DataLoader(traindataset, batch_size=50, shuffle=True)\n",
    "valid_loader = DataLoader(validdataset, batch_size=50, shuffle=True)\n",
    "test_loader  = DataLoader(testdataset, batch_size=50, shuffle=True)\n",
    "\n",
    "for i, (xbatch, ybatch) in enumerate(train_loader):\n",
    "    if i<2:\n",
    "        print(xbatch.shape, ybatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VivitModel, VivitConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vivit_config = VivitConfig(image_size=64, num_frames=30, hidden_size=1024, num_attention_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = VivitModel(vivit_config)(xbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vformer.encoder import SwinEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model      = ViTConvModel()\n",
    "criterion  = nn.MSELoss()\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs   = 10\n",
    "train_tsteps = 40 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print('Epoch: [{}/{}] | Batch: [{}/{}] | Loss: {}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_val_loss = 0.0\n",
    "    for x_val, y_val in valid_loader:\n",
    "        val_outputs = model(x_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "        total_val_loss += val_loss.item()\n",
    "    average_val_loss = total_val_loss / len(valid_loader)\n",
    "    print(f\"Validation Loss: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD ###\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.batchnorm  = nn.BatchNorm2d(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x, x, x)[0]\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_heads, num_layers):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding    = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.transformer_layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers) ])\n",
    "        self.fc                 = nn.Linear(embed_dim, in_channels)\n",
    "        self.batchnorm          = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        x = x.permute(1, 2, 0).view(x.size(1), -1, x.size(0))\n",
    "        x = self.fc(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.gelu(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
