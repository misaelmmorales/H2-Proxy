{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /work/08649/mmm6558/ls6/H2-Proxy/heterogeneity\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "def check_tensorflow_gpu():\n",
    "    sys_info = tf.sysconfig.get_build_info()\n",
    "    cuda_version, cudnn_version = sys_info['cuda_version'], sys_info['cudnn_version']\n",
    "    num_gpu_avail = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    gpu_name = device_lib.list_local_devices()[1].physical_device_desc[17:40]\n",
    "    print('... Checking Tensorflow Version ...')\n",
    "    print('Tensorflow built with CUDA?',  tf.test.is_built_with_cuda())\n",
    "    print(\"TF: {} | CUDA: {} | cuDNN: {}\".format(tf.__version__, cuda_version, cudnn_version))\n",
    "    print('# GPU available: {} ({})'.format(num_gpu_avail, gpu_name))\n",
    "    print(tf.config.list_physical_devices())\n",
    "    return None\n",
    "#check_tensorflow_gpu()\n",
    "\n",
    "print('Current Directory: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'h2dataf'\n",
    "file_list = [file for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "print(len(file_list))\n",
    "\n",
    "poro = np.zeros((256,256))\n",
    "perm = np.zeros((256,256))\n",
    "pres = np.zeros((60,256,256))\n",
    "satu = np.zeros((60,256,256))\n",
    "time = np.zeros((256,256))\n",
    "\n",
    "for i in range(1,1001):\n",
    "    datax = loadmat('{}/{}UHSS_0.mat'.format(data_folder,i), simplify_cells=True)\n",
    "    poro = datax['PORO']\n",
    "    perm = datax['PERMX']\n",
    "    for j in range(1,61):\n",
    "        data = loadmat('{}/{}UHSS_{}.mat'.format(data_folder,i,j), simplify_cells=True)\n",
    "        pres[j-1,:,:] = data['PRESSURE']\n",
    "        satu[j-1,:,:] = data['YMF_3']*data['SGAS']\n",
    "        time = np.ones((256,256))*j\n",
    "    np.savez('dataset/sample_{}.npz'.format(i), poro=poro, perm=perm, pres=pres, sat=satu, time=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'h2datag'\n",
    "file_list = [file for file in os.listdir(data_folder) if file.endswith('.mat')]\n",
    "print(len(file_list))\n",
    "\n",
    "poro = np.zeros((256,256))\n",
    "perm = np.zeros((256,256))\n",
    "pres = np.zeros((60,256,256))\n",
    "satu = np.zeros((60,256,256))\n",
    "time = np.zeros((256,256))\n",
    "\n",
    "for i in range(1,1001):\n",
    "    datax = loadmat('{}/{}UHSS_0.mat'.format(data_folder,i), simplify_cells=True)\n",
    "    poro = datax['PORO']\n",
    "    perm = datax['PERMX']\n",
    "    for j in range(1,61):\n",
    "        data = loadmat('{}/{}UHSS_{}.mat'.format(data_folder,i,j), simplify_cells=True)\n",
    "        pres[j-1,:,:] = data['PRESSURE']\n",
    "        satu[j-1,:,:] = data['YMF_3']*data['SGAS']\n",
    "        time = np.ones((256,256))*j\n",
    "    np.savez('Gdataset/sample_{}.npz'.format(i), poro=poro, perm=perm, pres=pres, sat=satu, time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(filters=embed_dim, kernel_size=patch_size, strides=patch_size, padding=\"same\")\n",
    "        self.flatten    = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n",
    "    \n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(input_dim=num_tokens, output_dim=self.embed_dim)\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "    def call(self, encoded_tokens):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens    = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME   = 'organmnist3d'\n",
    "BATCH_SIZE     = 32\n",
    "AUTO           = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE    = (60,128,128,3)\n",
    "NUM_CLASSES    = 2\n",
    "\n",
    "PATCH_SIZE     = (4,4,4)\n",
    "NUM_PATCHES    = (INPUT_SHAPE[0] // PATCH_SIZE[0]) **2\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS      = 16\n",
    "NUM_LAYERS     = 8\n",
    "\n",
    "EPOCHS         = 50\n",
    "LEARNING_RATE  = 1e-4\n",
    "WEIGHT_DECAY   = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_model(tubelet_embedder=TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE), \n",
    "                       positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM), \n",
    "                       input_shape=INPUT_SHAPE, transformer_layers=NUM_LAYERS, num_heads=NUM_HEADS, embed_dim=PROJECTION_DIM, \n",
    "                       layer_norm_eps=LAYER_NORM_EPS, attn_dropout=0.1, st_dropout=0.1):\n",
    "    K.clear_session()\n",
    "    def decoder_layer(inp, filters, kernel_size=3, last=False):\n",
    "        #x = layers.TimeDistributed(layers.SeparableConv2D(filters=filters, kernel_size=kernelsize, padding='same'))(inp)\n",
    "        if last:\n",
    "            x = layers.ConvLSTM2D(filters=filters, kernel_size=kernel_size, padding='same', return_sequences=False)(inp)\n",
    "            x = InstanceNormalization()(x)\n",
    "            x = tf.nn.gelu(x)\n",
    "            x = layers.SpatialDropout2D(rate=st_dropout)(x)\n",
    "            x = layers.Conv2DTranspose(filters=filters, kernel_size=kernel_size, padding='same', strides=2, activation='gelu')(x)\n",
    "        else:\n",
    "            x = layers.ConvLSTM2D(filters=filters, kernel_size=kernel_size, padding='same', return_sequences=True)(inp)\n",
    "            x = InstanceNormalization()(x)\n",
    "            x = tf.nn.gelu(x)\n",
    "            x = layers.TimeDistributed(layers.SpatialDropout2D(rate=st_dropout))(x)\n",
    "            x = layers.TimeDistributed(layers.Conv2DTranspose(filters=filters, kernel_size=kernel_size, padding='same', strides=2, activation='gelu'))(x)\n",
    "        return x\n",
    "\n",
    "    inputs          = layers.Input(shape=input_shape)\n",
    "    patches         = tubelet_embedder(inputs)\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "    \n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads, dropout=attn_dropout)(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=layer_norm_eps)(x2)\n",
    "        x3 = keras.Sequential([layers.Dense(units=embed_dim*4, activation=tf.nn.gelu),\n",
    "                               layers.Dense(units=embed_dim,   activation=tf.nn.gelu)])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "        \n",
    "    _ = layers.Reshape(target_shape = tuple([INPUT_SHAPE[i]//PATCH_SIZE[i] for i in range(len(PATCH_SIZE))] + [encoded_patches.shape[-1]]))(encoded_patches)\n",
    "    _ = decoder_layer(_, filters=64)\n",
    "    _ = decoder_layer(_, filters=32, last=True)\n",
    "\n",
    "    outputs = layers.Conv2DTranspose(filters=2, kernel_size=3, padding='same', activation='sigmoid')(_)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 60, 128, 128, 3)]    0         []                            \n",
      "                                                                                                  \n",
      " tubelet_embedding (Tubelet  (None, 15360, 128)           24704     ['input_1[0][0]']             \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " positional_encoder (Positi  (None, 15360, 128)           1966080   ['tubelet_embedding[0][0]']   \n",
      " onalEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 15360, 128)           256       ['positional_encoder[0][0]']  \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 15360, 128)           66048     ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 15360, 128)           0         ['multi_head_attention[0][0]',\n",
      "                                                                     'positional_encoder[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 15360, 128)           256       ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 15360, 128)           131712    ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 15360, 128)           0         ['sequential[0][0]',          \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 15360, 128)           256       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 15360, 128)           66048     ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 15360, 128)           0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 15360, 128)           256       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 15360, 128)           0         ['sequential_1[0][0]',        \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 15360, 128)           256       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 15360, 128)           66048     ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 15360, 128)           0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 15360, 128)           256       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 15360, 128)           0         ['sequential_2[0][0]',        \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 15360, 128)           256       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 15360, 128)           66048     ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 15360, 128)           0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 15360, 128)           256       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sequential_3 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 15360, 128)           0         ['sequential_3[0][0]',        \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 15360, 128)           256       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 15360, 128)           66048     ['layer_normalization_8[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 15360, 128)           0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_7[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 15360, 128)           256       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 15360, 128)           0         ['sequential_4[0][0]',        \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 15360, 128)           256       ['add_9[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 15360, 128)           66048     ['layer_normalization_10[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 15360, 128)           0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 15360, 128)           256       ['add_10[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 15360, 128)           0         ['sequential_5[0][0]',        \n",
      "                                                                     'add_10[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 15360, 128)           256       ['add_11[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 15360, 128)           66048     ['layer_normalization_12[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 15360, 128)           0         ['multi_head_attention_6[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_11[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 15360, 128)           256       ['add_12[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 15360, 128)           0         ['sequential_6[0][0]',        \n",
      "                                                                     'add_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 15360, 128)           256       ['add_13[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 15360, 128)           66048     ['layer_normalization_14[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 15360, 128)           0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_13[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 15360, 128)           256       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sequential_7 (Sequential)   (None, 15360, 128)           131712    ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 15360, 128)           0         ['sequential_7[0][0]',        \n",
      "                                                                     'add_14[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 15, 32, 32, 128)      0         ['add_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 15, 32, 32, 64)       442624    ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " instance_normalization (In  (None, 15, 32, 32, 64)       128       ['conv_lstm2d[0][0]']         \n",
      " stanceNormalization)                                                                             \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)     (None, 15, 32, 32, 64)       0         ['instance_normalization[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 15, 32, 32, 64)       0         ['tf.nn.gelu[0][0]']          \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 15, 64, 64, 64)       36928     ['time_distributed[0][0]']    \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 64, 64, 32)           110720    ['time_distributed_1[0][0]']  \n",
      "                                                                                                  \n",
      " instance_normalization_1 (  (None, 64, 64, 32)           64        ['conv_lstm2d_1[0][0]']       \n",
      " InstanceNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)   (None, 64, 64, 32)           0         ['instance_normalization_1[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " spatial_dropout2d_1 (Spati  (None, 64, 64, 32)           0         ['tf.nn.gelu_1[0][0]']        \n",
      " alDropout2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 128, 128, 32)         9248      ['spatial_dropout2d_1[0][0]'] \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 128, 128, 2)          578       ['conv2d_transpose_1[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4177250 (15.93 MB)\n",
      "Trainable params: 4177250 (15.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_vivit_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        train_videos = data[\"train_images\"]\n",
    "        valid_videos = data[\"val_images\"]\n",
    "        test_videos  = data[\"test_images\"]\n",
    "\n",
    "        train_labels = data[\"train_labels\"].flatten()\n",
    "        valid_labels = data[\"val_labels\"].flatten()\n",
    "        test_labels  = data[\"test_labels\"].flatten()\n",
    "\n",
    "    return ((train_videos, train_labels), (valid_videos, valid_labels), (test_videos, test_labels))\n",
    "\n",
    "info = medmnist.INFO[DATASET_NAME]\n",
    "prepared_dataset = download_and_prepare_dataset(info)\n",
    "(train_videos, train_labels) = prepared_dataset[0]\n",
    "(valid_videos, valid_labels) = prepared_dataset[1]\n",
    "(test_videos,  test_labels)  = prepared_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames:tf.Tensor, label:tf.Tensor):\n",
    "    frames = tf.image.convert_image_dtype(frames[..., tf.newaxis], tf.float32)\n",
    "    label  = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "def prepare_dataloader(videos:np.ndarray, labels:np.ndarray, loader_type:str=\"train\", batch_size:int=BATCH_SIZE):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "    dataloader = (dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE))\n",
    "    return dataloader\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "testloader  = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 34ms/step - loss: 0.9574 - accuracy: 0.8000 - top-5-accuracy: 0.9770\n",
      "Test: Accuracy=80.0000% | Top-5 Accuracy=97.7049%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    model = create_vivit_classifier(tubelet_embedder   = TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE),\n",
    "                                    positional_encoder = PositionalEncoder(embed_dim=PROJECTION_DIM))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "                           keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\")])\n",
    "\n",
    "    fit = model.fit(trainloader, \n",
    "                    epochs          = EPOCHS, \n",
    "                    validation_data = validloader, \n",
    "                    verbose         = 0)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print('Test: Accuracy={:.4f}% | Top-5 Accuracy={:.4f}%'.format(accuracy*100, top_5_accuracy*100))\n",
    "    return model, fit\n",
    "\n",
    "model, fit = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
